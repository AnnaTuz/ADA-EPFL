{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "- [Data Backups](#data-backups)\n",
    "- [Regex](#regex)\n",
    "- [Pandas](#pandas)\n",
    "- [Web parsing](#web-parsing)\n",
    "- [Visualisation](#visualisation)\n",
    "- [Pyspark](#pyspark)\n",
    "- [Applied ML](#applied-ml)\n",
    "- [Unsupervised Learning](#unsupervised-learning)\n",
    "- [Natural Language Processing](#natural-language-processing)\n",
    "- [Statistics](#statistics)\n",
    "- [Graph Networks](#graph-networks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T06:42:36.226390Z",
     "start_time": "2020-01-14T06:42:08.597861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Manipulation Libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "import math\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "import os, codecs, string, random\n",
    "from numpy.random import randint\n",
    "\n",
    "# Visualisation Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from datetime import datetime, date, time\n",
    "from dateutil.parser import parse\n",
    "from pandas.plotting import scatter_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Web parsing Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Big Data Libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.spatial.distance import jaccard\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# NLP Libraries\n",
    "import spacy, nltk, gensim, sklearn\n",
    "import pyLDAvis.gensim\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models.phrases import Phrases\n",
    "import re\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import itertools\n",
    "\n",
    "# Graphs\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "from community import community_louvain\n",
    "import collections\n",
    "from networkx.algorithms.community.centrality import girvan_newman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating rows of a column that each are a list\n",
    "words_for_chars = pd.concat([pd.Series(row[\"Character\"], row['Line'].split())\n",
    "                             for _, row in train_set.iterrows()]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data backups\n",
    "Dont forget to create a \"backup folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(index):\n",
    "    file = './backup/data_' + index + '.pkl'\n",
    "    with open(file, 'rb') as fp:\n",
    "        return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, index):\n",
    "    file = './backup/data_' + index + '.pkl'\n",
    "    with open(file, 'wb') as fp:\n",
    "        pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone\n",
    "index += 1\n",
    "save_data(bbt, str(index))\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-13T10:22:53.206209Z",
     "start_time": "2020-01-13T10:22:53.188699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find word in text\n",
    "def word_in_text(words, text):\n",
    "    words = re.sub('s+','s*', '|'.join(words)) # Find pattern, replace with other, the list\n",
    "    text = text.lower()\n",
    "    match = re.search(words, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete substring until certain character\n",
    "x : x[x.find(':'):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    Find all lines containing a word <br>\n",
    "    Counting tweets containing a word\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually apply the regexp\n",
    "keywords = ['open access', 'open science', 'ipython', 'open data', 'reproducible research','epfl']\n",
    "for w in keywords:\n",
    "    tweets[w] = tweets['text'].apply(lambda tweet: word_in_text([w], tweet))\n",
    "\n",
    "tweets_by_kw = pd.Series([tweets[w].value_counts()[True] for w in keywords], index=keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby function\n",
    "df.groupby('A').apply(lambda x: x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    Standardise times\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating date\n",
    "datetime(1970, 1, 1) # yyyy - mm -dd\n",
    "\n",
    "# Parsing an entire column of date\n",
    "df.column.apply(lambda d: datetime.strptime(d, '%m/%d/%y %H:%M')).head(10)\n",
    "# Filter per month\n",
    "df[df.column.dt.month==2].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">\n",
    "Rotates / reshapes DataFrames (wide ⇄ long).\n",
    "\n",
    "\n",
    "**stack** — Moves column labels into a lower-level row index (wide → long).\n",
    "Example: `df.stack()` turns columns into an inner index and typically returns a Series.\n",
    "\n",
    "**unstack** — Inverse of `stack`: moves an index level into columns (long → wide).\n",
    "Example: If `s = df.stack()`, then `s.unstack()` restores the original `df` (when pairs are unique).\n",
    "\n",
    "**pivot** — Reshapes long-form columns into a wide table by specifying `index`, `columns`, and `values`.\n",
    "Requires unique (`index`,`columns`) pairs; otherwise use `pivot_table` with an aggregation.\n",
    "Example: `df.pivot(index='date', columns='var', values='val')` creates one column per `var`.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stack() # Columns into rows\n",
    "df.unstack() # Reverts the effect\n",
    "\n",
    "df_wide = df.pivot(index='id_column', columns='column to pivot', values='twstrs').head()\n",
    "\n",
    "df.explode(column=column)\n",
    "\n",
    "# crosstab for contingency tables\n",
    "pd.crosstab(df['col1'], df['col2'], margins=True)\n",
    "\n",
    "# merge / join examples\n",
    "pd.merge(left_df, right_df, how='left', on='key')\n",
    "left_df.join(right_df.set_index('key'), on='key', how='inner')\n",
    "\n",
    "# concat multiple dataframes (stack vertically/horizontally)\n",
    "pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# hstack porcodio\n",
    "X2 = np.hstack((X, merged_df_[[\"PP\", \"NN\", \"PN\", \"N\", \"P\"]].values))\n",
    "\n",
    "# reshape long -> wide / wide -> long\n",
    "df.melt(id_vars=['id'], value_vars=['A','B'], var_name='variable', value_name='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index and renaming\n",
    "df = df.rename(columns={'Historical Significance': 'Role'})\n",
    "df.set_index('index_column', inplace=True)\n",
    "\n",
    "df.set_index(pd.to_datetime(df['date'])).resample('M').sum()\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples: grouping & aggregations\n",
    "df.groupby('group_col').agg({'val1': ['mean', 'sum', 'sem'], 'val2': 'max'})\n",
    "\n",
    "# Named aggregations (pandas 0.25+)\n",
    "df.groupby('group_col').agg( mean_val1=('val1', 'mean'), sum_val2=('val2', 'sum'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group size and count\n",
    "df.groupby('group_col').size()\n",
    "df.groupby('group_col')['val1'].count()\n",
    "df['count_values'] = df['col'].value_counts()  # returns Series (use map to add to df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform vs apply: keep index alignment\n",
    "df['val1_pct_of_group'] = df.groupby('group_col')['val1'].transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumsum, rank, standardized within group\n",
    "df['cum'] = df.groupby('group_col')['val1'].cumsum()\n",
    "df['rank'] = df.groupby('group_col')['val1'].rank(ascending=False)\n",
    "\n",
    "# pivot_table with aggregation and fill_value\n",
    "pd.pivot_table(df, index='date', columns='category', values='value', aggfunc='mean', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful selectors\n",
    "df.query(\"colA > 10 and colB == 'X'\")\n",
    "df[df['col'].isin(['a','b','c'])]\n",
    "\n",
    "df.nlargest(5, 'score')\n",
    "df.nsmallest(5, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series / rolling windows\n",
    "df['rolling_mean'] = df['value'].rolling(window=7, min_periods=1).mean()\n",
    "df['ewm'] = df['value'].ewm(span=7).mean() # exponential moving window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates\n",
    "df['is_dup'] = df.duplicated(subset=['col1','col2'])\n",
    "df.drop_duplicates(subset=['col1','col2'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = requests.get('http://worldtimeapi.org/api/timezone/Europe/Zurich')\n",
    "r.json()\n",
    "r.text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.post('https://httpbin.org/post', data=payload)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "soup.h1\n",
    "soup.title.string\n",
    "all_links = soup.find_all('a')\n",
    "\n",
    "for link in all_links:\n",
    "    if(not link.get('href').startswith('http://dblp.uni-trier.de/')\n",
    "       and link.get('href').startswith('http')):  # just an example, you need more checks\n",
    "        external_links += 1\n",
    "\n",
    "publications_wrappers = soup.find_all('li', class_='entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-log plot - Log axis with pyplot\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattering plots to observe relationships between arguments\n",
    "df.plot(kind='scatter', x='newspaper', y='sales', ax=axs[2], grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms / density\n",
    "plt.hist(data, bins=30, color='skyblue', edgecolor='k')\n",
    "plt.xlabel('value')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(x, y, label='line')\n",
    "ax.scatter(x, y, c='C1', alpha=0.7, label='points')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Line + Scatter')\n",
    "ax.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot (categorical counts)\n",
    "counts = df['cat'].value_counts()\n",
    "counts.plot(kind='bar', color='coral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot / violinplot (with seaborn)\n",
    "sns.set_theme(style='whitegrid')\n",
    "sns.boxplot(x='category', y='value', data=df)\n",
    "sns.violinplot(x='category', y='value', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE / hist with seaborn\n",
    "sns.histplot(df['value'], kde=True, bins=40)\n",
    "sns.kdeplot(df['value'], shade=True)\n",
    "\n",
    "# Countplot / barplot\n",
    "sns.countplot(x='category', data=df)\n",
    "sns.barplot(x='category', y='value', data=df, ci=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter with regression line\n",
    "sns.regplot(x='xcol', y='ycol', data=df, scatter_kws={'s':10}, line_kws={'color':'red'})\n",
    "\n",
    "# ECDF (used in exams)\n",
    "sns.ecdfplot(data=df['degree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot / jointplot / heatmap\n",
    "sns.pairplot(df[['a','b','c']])\n",
    "sns.jointplot(x='a', y='b', data=df, kind='hex')\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='vlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplots grid\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12,3))\n",
    "axs[0].plot(x, y1)\n",
    "axs[1].plot(x, y2)\n",
    "axs[2].plot(x, y3)\n",
    "for ax in axs:\n",
    "    ax.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations and reference lines\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.axhline(0.5, color='gray', linestyle=':')\n",
    "ax.annotate('peak', xy=(x[idx], y[idx]), xytext=(x[idx]+1, y[idx]+1),\n",
    "            arrowprops=dict(arrowstyle='->'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colorbar for imshow/heatmap\n",
    "im = ax.imshow(matrix, cmap='viridis')\n",
    "fig.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Bars (always show them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive error-bar examples\n",
    "import numpy as np\n",
    "from scipy.stats import sem\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1) Simple errorbar (symmetric)\n",
    "x = np.arange(5)\n",
    "y = np.array([3., 5., 2., 8., 7.])\n",
    "yerr = np.array([0.5, 0.8, 0.2, 1.0, 0.6])\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.errorbar(x, y, yerr=yerr, fmt='o-', color='C0',\n",
    "             ecolor='gray', elinewidth=2, capsize=4, capthick=1,\n",
    "             label='mean ± SE')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2) Bar plot with error bars (means + SEM)\n",
    "labels = ['A','B','C']\n",
    "means = np.array([2.3, 3.1, 4.0])\n",
    "sems = np.array([0.2, 0.25, 0.15])\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.bar(labels, means, yerr=sems, capsize=6, color='skyblue', edgecolor='k')\n",
    "plt.ylabel('value')\n",
    "plt.title('Mean ± SEM')\n",
    "plt.show()\n",
    "\n",
    "# 3) Asymmetric errors (lower, upper)\n",
    "lower = np.array([0.4,0.2,0.6])\n",
    "upper = np.array([0.8,0.3,0.4])\n",
    "asym_err = [lower, upper]\n",
    "plt.figure()\n",
    "plt.errorbar([0,1,2], means, yerr=asym_err, fmt='o', capsize=5)\n",
    "plt.show()\n",
    "\n",
    "# 4) Compute mean ± 95% CI from raw data (pandas groupby example)\n",
    "# assumes `df` with columns 'group' and 'value'\n",
    "# from scipy.stats import sem  # already imported above\n",
    "grouped = df.groupby('group')['value']\n",
    "means = grouped.mean()\n",
    "sems = grouped.apply(lambda x: sem(x, nan_policy='omit'))\n",
    "ci95 = sems * 1.96\n",
    "ax = means.plot(kind='bar', yerr=ci95, capsize=5, rot=0)\n",
    "ax.set_ylabel('mean value')\n",
    "plt.title('Group means with 95% CI')\n",
    "plt.show()\n",
    "\n",
    "# 5) Seaborn helpers (point estimates + CI)\n",
    "# pointplot shows mean with CI (default 95%)\n",
    "sns.pointplot(data=df, x='group', y='value', capsize=.1)\n",
    "# barplot also supports ci (use ci=None to turn off)\n",
    "sns.barplot(data=df, x='group', y='value', ci=95, capsize=.1)\n",
    "plt.show()\n",
    "\n",
    "# 6) Error band for time series (rolling +/- std)\n",
    "# assumes `ts` is a DataFrame or Series with a DatetimeIndex and column 'value'\n",
    "rolling_mean = ts['value'].rolling(7, min_periods=1).mean()\n",
    "rolling_std = ts['value'].rolling(7, min_periods=1).std()\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(ts.index, rolling_mean, label='7-day mean')\n",
    "plt.fill_between(ts.index, rolling_mean - rolling_std, rolling_mean + rolling_std,\n",
    "                 color='C0', alpha=0.2, label='±1 std')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tips and options:\n",
    "# - use `capsize` to show caps, `ecolor` to style error lines\n",
    "# - `elinewidth` and `capthick` control thickness\n",
    "# - pass asymmetric errors as [lower, upper]\n",
    "# - for large datasets, consider seaborn's aggregators (pointplot/barplot) or bootstrap CIs\n",
    "# - to show only every nth errorbar, use `errorevery` in plt.errorbar\n",
    "# Example: plt.errorbar(x, y, yerr=yerr, errorevery=2)\n",
    "# - to represent CI visually, `fill_between` often looks cleaner than many caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T17:04:15.630943Z",
     "start_time": "2020-01-11T17:03:53.150550Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setMaster(\"local[*]\").setAll([\n",
    "                                   ('spark.executor.memory', '12g'),  # find\n",
    "                                   ('spark.driver.memory','4g'), # your\n",
    "                                   ('spark.driver.maxResultSize', '2G') # setup\n",
    "                                  ])\n",
    "# create the session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT AirCraftType, count(*) MissionsCount\n",
    "FROM Bombing_Operations bo\n",
    "JOIN Aircraft_Glossary ag\n",
    "ON bo.AirCraft = ag.AirCraft\n",
    "GROUP BY AirCraftType\n",
    "ORDER BY MissionsCount DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping function\n",
    "liste.map(lambda x : x*2)\n",
    "\n",
    "# Filter function\n",
    "liste.filter(lambda x : x < 2)\n",
    "\n",
    "# Flatmap\n",
    "liste.flatmap(lambda x : [x, x*10])\n",
    "\n",
    "# Sample\n",
    "liste.sample(withReplacementBoolean, sampledFraction, seed)\n",
    "\n",
    "# Union\n",
    "liste.union(dataset2)\n",
    "\n",
    "# Intersection\n",
    "liste.intersection(dataset2)\n",
    "\n",
    "# Distinct\n",
    "liste.distinct()\n",
    "\n",
    "# Groupby\n",
    "dic.groupByKey()\n",
    "\n",
    "# Reduce\n",
    "dic.reduceByKey(sum)\n",
    "\n",
    "# Sort by key\n",
    "dic.sortByKey()\n",
    "\n",
    "# Join {(1,a), (2,b)}.join({(1,A), (1,X)}) → {(1, (a,A)), (1, (a,X))}\n",
    "dic.join(dic2)\n",
    "\n",
    "# collect - Return all elements of the dataset as an array\n",
    "data.collect()\n",
    "\n",
    "# Count elements in the dataset\n",
    "data.count()\n",
    "\n",
    "# Take the first n elements\n",
    "data.take(n)\n",
    "\n",
    "# Saving\n",
    "data.saveAsTextFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing data\n",
    "df.fillna({'col': 0})\n",
    "df.dropna(subset=['important_col'])\n",
    "df.interpolate(method='time')  # if index is datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation from categorical to numerical -> One hot encoding\n",
    "X =  pd.get_dummies(pokemon_features[pokemon_features.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaler - Scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lin_reg = LinearRegression()  # create the model\n",
    "lin_reg.fit(X, y)  # train it\n",
    "lin_reg.coef_ # Coefficient values for each attribute\n",
    "\n",
    "lr = LinearRegression()\n",
    "# Function for cross validation\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")\n",
    "\n",
    "\n",
    "\n",
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "logistic.fit(X, y)\n",
    "logistic.predict([[25, 100, 0, 1]])\n",
    "logistic.predict_proba([[25, 100, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(Cs=[], cv=10, random_state=0).fit(X, y)\n",
    "clf.predict(X[:2, :])\n",
    "clf.predict_proba(X[:2, :]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbours - Classification\n",
    "Distance measures available:\n",
    "* euclidian\n",
    "* manhattan\n",
    "* chebyshev\n",
    "* minkowski (default)\n",
    "* wminkowski\n",
    "* seuclidean\n",
    "* mahalanobis\n",
    "* jaccard\n",
    "* hamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X, y)\n",
    "knn.predict([[test_instance]])\n",
    "knn.predict_proba([[0.9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbours - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=2)\n",
    "knn.fit(X, y)\n",
    "knn.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "max_depth = None\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "# Feature importance\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(10), importances[indices[:10]], yerr=std[indices[:10]], align=\"center\")\n",
    "plt.xticks(range(10), indices[:10])\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "cross_val_score(dt, iris.data, iris.target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-10T21:19:52.988155Z",
     "start_time": "2020-01-10T21:19:52.913429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Entropy\n",
    "def H(p, n):\n",
    "    temp1 = (p / (p + n)) * np.log2(p / (p + n))\n",
    "    temp2 = (n / (p + n)) * np.log2(n / (p + n))\n",
    "    return - temp1 - temp2\n",
    "\n",
    "def entropy(dataframe, ps, ns):\n",
    "    '''\n",
    "    Dataframe should be two columns :\n",
    "    1: attribute to compute the entropy on, called \"attribute\"\n",
    "    2: label of the rows, called \"label\"\n",
    "    -> Labels are assumed to be 0 (negative) or 1 (positive)\n",
    "    '''\n",
    "    values = dataframe['attribute'].unique()\n",
    "    entropy_value = 0\n",
    "    for val in values:\n",
    "        temp = dataframe[dataframe['attribute'] == val]\n",
    "        pos = len(temp[temp['label'] == 1])\n",
    "        neg = len(temp[temp['label'] == 0])\n",
    "        entropy_value += ((pos + neg) / (ps + ns)) * H(pos, neg)\n",
    "    return entropy_value\n",
    "\n",
    "def gain(dataframe):\n",
    "    ps = len(dataframe[dataframe['label'] == 1])\n",
    "    ns = len(dataframe[dataframe['label'] == 0])\n",
    "\n",
    "    return H(ps, ns) - entropy(dataframe, ps, ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Features\n",
    "coefficient = pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features and label\n",
    "# Estimated mutual information between each feature and the target\n",
    "mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3,\n",
    "                    copy=True, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = ttest_ind(a, b, axis=0, equal_var=True, nan_policy='propagate')[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = wilcoxon(x, y=None, zero_method='wilcox', correction=False, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, p = kstest(rvs, cdf, args=(), N=20, alternative='two-sided', mode='approx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Extract TP, TN, FP, FN\n",
    "tn, fp, fn, tp = cm.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence Interval\n",
    "np.quantile( np.array(BF), q=[0.025, 0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE: Mean squared error\n",
    "mean_squared_error(y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision cross cal\n",
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\") # scoring=\"recall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 scores - R² Scores : 0-> Bad model, 1-> Good model\n",
    "r2_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def get_accuracy(tp, tn, fp, fn):\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# Precision\n",
    "def get_precision(tp, tn, fp, fn):\n",
    "    return (tp) / (tp + fp)\n",
    "\n",
    "# Recall\n",
    "def get_recall(tp, tn, fp, fn):\n",
    "    return (tp) / (tp + fn)\n",
    "\n",
    "# F1 score - F score\n",
    "def get_f1score(tp, tn, fp, fn):\n",
    "    p = get_precision(tp, tn, fp, fn)\n",
    "    r = get_recall(tp, tn, fp, fn)\n",
    "\n",
    "    return 2 * ((p * r) / (p + r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "y_true = df[\"VOT\"].map({-1: 0, 1: 1})\n",
    "a_y_preds = a_classifier(df)\n",
    "b_y_preds = b_classifier(df)\n",
    "\n",
    "a_fpr, a_tpr, threshold = metrics.roc_curve(y_true, a_y_preds)\n",
    "a_roc_auc = metrics.auc(a_fpr, a_tpr)\n",
    "\n",
    "b_fpr, b_tpr, threshold = metrics.roc_curve(y_true, b_y_preds)\n",
    "b_roc_auc = metrics.auc(b_fpr, b_tpr)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 10))\n",
    "\n",
    "ax[0].set_title('A Classifier ROC AUC Curve')\n",
    "ax[0].plot(a_fpr, a_tpr, 'b', label = 'AUC = %0.2f' % a_roc_auc)\n",
    "ax[0].legend(loc = 'lower right')\n",
    "ax[0].plot([0, 1], [0, 1],'r--')\n",
    "ax[0].set_xlim([0, 1])\n",
    "ax[0].set_ylim([0, 1])\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "\n",
    "ax[1].set_title('B Classifier ROC AUC Curve')\n",
    "ax[1].plot(b_fpr, b_tpr, 'b', label = 'AUC = %0.2f' % b_roc_auc)\n",
    "ax[1].legend(loc = 'lower right')\n",
    "ax[1].plot([0, 1], [0, 1],'r--')\n",
    "ax[1].set_xlim([0, 1])\n",
    "ax[1].set_ylim([0, 1])\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "euc_dist = euclidean(x, y, w=None)\n",
    "\n",
    "# Cosine distance\n",
    "cos_dist = cosine(x, y, w=None)\n",
    "\n",
    "# Jaccard distance\n",
    "jac_dist = jaccard(x, y, w=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means algorithm\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "kmeans.labels_ # Cluters created\n",
    "kmeans.predict([[0, 0], [12, 3]])\n",
    "kmeans.cluster_centers_ # Centers of the predicted clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=3, min_samples=2).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\">    \n",
    "    All functions here are related to language processing on a series of books. <br>\n",
    "    The variable \"books\" is considered to be a list of several books\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete new lines\n",
    "books = [\" \".join(b.split()) for b in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into raw text / spacy object\n",
    "doc = nlp(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens\n",
    "tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Lemmas\n",
    "for token in doc:\n",
    "    print(token.text,'--->',token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of Speech Tagging - POS\n",
    "# Tag is more specific\n",
    "pos_tagged = [(token.text, token.pos_, token.tag_) for token in doc]\n",
    "\n",
    "print(example,'\\n')\n",
    "print(pos_tagged)\n",
    "\n",
    "print(spacy.explain('CCONJ')) # Explanation of a grammatical class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent words\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams\n",
    "bigram = Phrases(docs, min_count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF ~ tf idf\n",
    "tfids_vec = TfidfVectorizer()\n",
    "X = tfids_vec.fit_transform(corpus)# Corpus : list of strings\n",
    "print(tfids_vec.get_feature_names())\n",
    "\n",
    "# Bag of Words - Count vectoriser\n",
    "bow_vec = CountVectorizer(ngram_range=(2, 2))\n",
    "X = bow_vec.fit_transform(corpus)\n",
    "print(bow_vec.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Semantic Analysis - LSA\n",
    "lsa = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "lsa.fit(X)\n",
    "print(lsa.explained_variance_ratio_)\n",
    "print(lsa.explained_variance_ratio_.sum())\n",
    "print(lsa.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Dirichlet Allocation - LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "print(lda.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Types\n",
    "* PERSON People, including fictional.\n",
    "* NORP Nationalities or religious or political groups.\n",
    "* FAC Buildings, airports, highways, bridges, etc.\n",
    "* ORG Companies, agencies, institutions, etc.\n",
    "* GPE Countries, cities, states.\n",
    "* LOC Non-GPE locations, mountain ranges, bodies of water.\n",
    "* PRODUCT Objects, vehicles, foods, etc. (Not services.)\n",
    "* EVENT Named hurricanes, battles, wars, sports events, etc.\n",
    "* WORK_OF_ART Titles of books, songs, etc.\n",
    "* LAW Named documents made into laws.\n",
    "* LANGUAGE Any named language.\n",
    "* DATE Absolute or relative dates or periods.\n",
    "* TIME Times smaller than a day.\n",
    "* PERCENT Percentage, including \"%\".\n",
    "* MONEY Monetary values, including unit.\n",
    "* QUANTITY Measurements, as of weight or distance.\n",
    "* ORDINAL \"first\", \"second\", etc.\n",
    "* CARDINAL Numerals that do not fall under another type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stop_words = [token.text for token in doc if token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance\n",
    "variance = np.var(x)\n",
    "\n",
    "# Standard Deviation\n",
    "std = np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random number\n",
    "seed = randint(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample dataset\n",
    "df.sample(frac=1, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstraps - Bootstrapping - list\n",
    "mean_bootstraps = []\n",
    "yerrs_list1 = []\n",
    "yerrs_list2 = []\n",
    "bootstrap_datasets = []\n",
    "for bootstrap_dataset in df['attribute'].unique():\n",
    "    bootstrap_datasets.append(bootstrap_dataset)\n",
    "    bootstrap_scores = []\n",
    "    n = 1000\n",
    "    df = df[df['attribute'] == bootstrap_dataset]\n",
    "    for i in range(n):\n",
    "        # Extraction of the sample\n",
    "        indices_sample = np.random.choice(list(range(len(df))),\n",
    "                                          len(df), replace=True)\n",
    "        df_sample = df.iloc[indices_sample]\n",
    "\n",
    "        # Bootstrap list\n",
    "        bootstrap_scores.append(np.mean(df_sample['nr_words']))\n",
    "\n",
    "\n",
    "    # Computation on the whole dataset\n",
    "    bootstrap_scores = np.sort(bootstrap_scores)\n",
    "    mean_bootstrap = np.mean(bootstrap_scores)\n",
    "    mean_bootstraps.append(mean_bootstrap)\n",
    "    lower_bootstrap = (bootstrap_scores - mean_bootstrap)[int(n * 0.025)]\n",
    "    higher_bootstrap = (bootstrap_scores - mean_bootstrap)[int(n * 0.025)]\n",
    "\n",
    "    yerrs = [(lower_bootstrap), (higher_bootstrap)]\n",
    "    yerrs_list1.append(yerrs[0])\n",
    "    yerrs_list2.append(yerrs[1])\n",
    "\n",
    "\n",
    "yerrs_list = [yerrs_list1, yerrs_list2]\n",
    "plt.figure(figsize=(25, 8))\n",
    "# Can change mean and yerrs to array for several barplots\n",
    "plt.bar(x = list(range(19)), height = mean_bootstraps, yerr=yerrs_list, color='paleturquoise')\n",
    "plt.xticks(list(range(19)), bootstrap_datasets)\n",
    "plt.xlim(-0.5, 20.5)\n",
    "plt.title('')\n",
    "plt.xlabel('Mean (*) and 95% confidence interval (-)')\n",
    "plt.ylabel(\"Mean square error from the bootstrap sampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstraps - Bootstrapping - One value\n",
    "bootstrap_scores = []\n",
    "n = 10000\n",
    "n_samples = 56\n",
    "for i in range(n):\n",
    "    # Extraction of the sample\n",
    "    indices_sample_grass = np.random.choice(list(range(len(grasses))),\n",
    "                                      n_samples, replace=True)\n",
    "    indices_sample_rock = np.random.choice(list(range(len(rocks))),\n",
    "                                      n_samples, replace=True)\n",
    "    grass_sample = grasses.iloc[indices_sample_grass]\n",
    "    rock_sample = rocks.iloc[indices_sample_rock]\n",
    "\n",
    "    stat, p = ttest_ind(grass_sample[\"attack\"], rock_sample[\"attack\"], axis=0,\n",
    "                        equal_var=True, nan_policy='propagate')\n",
    "    # Bootstrap list\n",
    "    bootstrap_scores.append(p)\n",
    "\n",
    "\n",
    "# Computation on the whole dataset\n",
    "bootstrap_scores = np.sort(bootstrap_scores)\n",
    "mean_bootstrap = np.mean(bootstrap_scores)\n",
    "lower_bootstrap = (bootstrap_scores - mean_bootstrap)[int(n * 0.025)]\n",
    "higher_bootstrap = (bootstrap_scores - mean_bootstrap)[int(n * 0.025)]\n",
    "\n",
    "std = np.std(bootstrap_scores)\n",
    "\n",
    "yerrs = [[lower_bootstrap, mean_bootstrap - std], [higher_bootstrap, mean_bootstrap + std]]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Can change mean and yerrs to array for several barplots\n",
    "plt.bar(x = [0, 1], height = mean_bootstrap, yerr=yerrs, color='paleturquoise')\n",
    "plt.xticks([0, 1], ['95% confidence interval', 'STD'])\n",
    "plt.xlabel('p-value of the t test between the attack value of grass and rock pokemons')\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.title('Bootstrapped p-Value')\n",
    "plt.xlabel('Mean (*) and 95% confidence interval (-)')\n",
    "plt.ylabel(\"Mean square error from the bootstrap sampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph\n",
    "G = nx.Graph() # undirected\n",
    "di_G = nx.DiGraph() # directed\n",
    "\n",
    "# Add Nodes\n",
    "G.add_node(1)\n",
    "G.add_nodes_from(range(2,9))\n",
    "\n",
    "# Add Edges\n",
    "G.add_edge(1,2)\n",
    "edges = [(2,3), (1,3), (4,1), (4,5), (5,6), (5,7), (6,7), (7,8), (6,8)]\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges from pandas\n",
    "G = nx.from_pandas_edgelist(edges, 'Source', 'Target',\n",
    "                                  edge_attr=None, create_using = nx.Graph())\n",
    "\n",
    "# Add nodes from pandas\n",
    "# add node attributes by passing dictionary of type name -> attribute\n",
    "nx.set_node_attributes(G, nodes['attributes'].to_dict(), 'attributes' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Graph\n",
    "erG = nx.gnm_random_graph(n, m) # n:nodes - m:edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Nodes\n",
    "G.nodes()\n",
    "\n",
    "# Get Information\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Graph\n",
    "nx.draw_spring(G, with_labels=True,  alpha = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T20:01:12.510442Z",
     "start_time": "2020-01-11T20:01:12.486254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Degree distrbution\n",
    "def plot_degree_distribution(G):\n",
    "    degrees = {}\n",
    "    for node in G.nodes():\n",
    "        degree = G.degree(node)\n",
    "        if degree not in degrees:\n",
    "            degrees[degree] = 0\n",
    "        degrees[degree] += 1\n",
    "    sorted_degree = sorted(degrees.items())\n",
    "    deg = [k for (k,v) in sorted_degree]\n",
    "    cnt = [v for (k,v) in sorted_degree]\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(deg, cnt, width=0.80, color='plum')\n",
    "    plt.title(\"Degree Distribution\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Degree\")\n",
    "    ax.set_xticks([d+0.05 for d in deg])\n",
    "    ax.set_xticklabels(deg)\n",
    "\n",
    "# Graph properties\n",
    "def describe_graph(G):\n",
    "    print(nx.info(G))\n",
    "    if nx.is_connected(G):\n",
    "        print(\"Avg. Shortest Path Length: %.4f\" %nx.average_shortest_path_length(G))\n",
    "        print(\"Diameter: %.4f\" %nx.diameter(G)) # Longest shortest path\n",
    "    else:\n",
    "        print(\"Graph is not connected\")\n",
    "        print(\"Diameter and Avg shortest path length are not defined!\")\n",
    "    print(\"Sparsity: %.4f\" %nx.density(G))  # #edges/#edges-complete-graph\n",
    "    # #closed-triplets(3*#triangles)/#all-triplets\n",
    "    print(\"Global clustering coefficient aka Transitivity: %.4f\" %nx.transitivity(G))\n",
    "\n",
    "# Helper function for visualizing the graph\n",
    "def visualize_graph(G, with_labels=True, k=None, alpha=0.6, node_shape='o'):\n",
    "    #nx.draw_spring(G, with_labels=with_labels, alpha = alpha)\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    pos = nx.spring_layout(G, k=k)\n",
    "    if with_labels:\n",
    "        lab = nx.draw_networkx_labels(G, pos, labels=dict([(n, n) for n in G.nodes()]))\n",
    "    ec = nx.draw_networkx_edges(G, pos, alpha=alpha)\n",
    "    nc = nx.draw_networkx_nodes(G, pos, nodelist=G.nodes(), node_color='powderblue', node_shape=node_shape)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circular graph\n",
    "nx.draw_circular(G, with_labels=True,  node_color='powderblue', alpha = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity\n",
    "print(\"Network sparsity: %.4f\" %nx.density(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if connected graph\n",
    "print(nx.is_connected(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of components\n",
    "comp = list(nx.connected_components(G))\n",
    "print('The graph contains', len(comp), 'connected components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largest component\n",
    "largest_comp = max(comp, key=len)\n",
    "percentage_lcc = len(largest_comp)/G.number_of_nodes() * 100\n",
    "print('The largest component has', len(largest_comp), 'nodes',\n",
    "      'accounting for %.2f'% percentage_lcc, '% of the nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortest path\n",
    "path = nx.shortest_path(G, source=\"source node\", target=\"target node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longest short path - diameter of the graph\n",
    "temp_graph = G.subgraph(largest_comp)\n",
    "print(\"The diameter of the largest connected component is\",\n",
    "      nx.diameter(G))\n",
    "print(\"The avg shortest path length of the largest connected component is\",\n",
    "      nx.average_shortest_path_length(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of all possible triangles - Transitivity - Triadic closure\n",
    "print('%.4f' %nx.transitivity(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering coefficient\n",
    "print(nx.clustering(G, ['node1', 'node2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgraph\n",
    "subgraph_node1 = G.subgraph(['Node 1'] + list(G.neighbors('Node 1')))\n",
    "nx.draw_spring(G, with_labels=True)\n",
    "nx.draw_circular(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degrees and see importance\n",
    "degrees = dict(G.degree(G.nodes()))\n",
    "sorted_degree = sorted(degrees.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# And the top 5 most popular quakers are..\n",
    "for nodeName, degree in sorted_degree[:5]:\n",
    "    print(nodeName, 'who is', G.node[nodeName]['Role'], 'knows', degree, 'people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Distribution Scatter plot\n",
    "degree_seq = [d[1] for d in sorted_degree]\n",
    "degreeCount = collections.Counter(degree_seq)\n",
    "degreeCount = pd.DataFrame.from_dict( degreeCount, orient='index').reset_index()\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(degreeCount['index'], degreeCount[0], 'o', c='blue', markersize= 4)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Degree')\n",
    "plt.title('Degree distribution for the Quaker network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree of Centrality Katz\n",
    "degrees = dict(G.degree(G.nodes()))\n",
    "\n",
    "katz = nx.katz_centrality(G)\n",
    "nx.set_node_attributes(G, katz, 'katz')\n",
    "sorted_katz = sorted(katz.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# And the top 5 most popular quakers are..\n",
    "for nodeName, katzc in sorted_katz[:5]:\n",
    "    print(nodeName, 'who is', G.node[nodeName]['Role'], 'has katz-centrality: %.3f' %katzc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute betweenness centrality\n",
    "betweenness = nx.betweenness_centrality(G)\n",
    "# Assign the computed centrality values as a node-attribute in your network\n",
    "nx.set_node_attributes(G, betweenness, 'betweenness')\n",
    "sorted_betweenness = sorted(betweenness.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "for nodeName, bw in sorted_betweenness[:5]:\n",
    "    print(nodeName, 'who is', G.node[nodeName]['Role'], 'has betweeness: %.3f' %bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree centrality heatmap\n",
    "# similar pattern\n",
    "list_nodes =list(G.nodes())\n",
    "list_nodes.reverse()   # for showing the nodes with high betweeness centrality\n",
    "pos = nx.spring_layout(G)\n",
    "ec = nx.draw_networkx_edges(G, pos, alpha=0.1)\n",
    "nc = nx.draw_networkx_nodes(G, pos, nodelist=list_nodes,\n",
    "                            node_color=[G.nodes[n][\"betweenness\"] for n in list_nodes],\n",
    "                            with_labels=False, alpha=0.8, node_shape = '.')\n",
    "plt.colorbar(nc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girvan Newman\n",
    "comp = girvan_newman(G)\n",
    "it = 0\n",
    "for communities in itertools.islice(comp, 4):\n",
    "    it +=1\n",
    "    print('Iteration', it)\n",
    "    print(tuple(sorted(c) for c in communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain\n",
    "partition = community_louvain.best_partition(G)\n",
    "# add it as an attribute to the nodes\n",
    "for n in G.nodes:\n",
    "    G.nodes[n][\"louvain\"] = partition[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G, k=0.2)\n",
    "ec = nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "nc = nx.draw_networkx_nodes(G, pos, nodelist=G.nodes(),\n",
    "                            node_color=[G.nodes[n][\"louvain\"] for n in G.nodes],\n",
    "                            with_labels=False, node_size=100, cmap=plt.cm.jet)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterNode = partition['Node']\n",
    "# Take all the nodes that belong to James' cluster\n",
    "members_c = [q for q in G.nodes if partition[q] == clusterNode]\n",
    "# get info about these quakers\n",
    "for nodeName in members_c:\n",
    "    print(nodeName, 'who is', G.node[nodeName]['Role'], 'and died in ',\n",
    "          G.node[nodeName]['Deathdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homophily\n",
    "# for categorical attributes\n",
    "nx.attribute_assortativity_coefficient(G, 'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if edge\n",
    "G.has_edge(a, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada-25-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
